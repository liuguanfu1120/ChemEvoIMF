{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created On April 30, 2024\n",
    "Last modified on April 30, 2024\n",
    "\n",
    "Description: \n",
    "To Process and interpolate the stellar yields from NuPyCEE, https://github.com/NuGrid/NuPyCEE/tree/master\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import h5py\n",
    "sys.path.insert(0, '/Users/liuguanfu/Workspace/SAS-21/targets/MRK1216/spex/jupyter/IMF/chemevoimf')\n",
    "import utils\n",
    "from scipy import interpolate\n",
    "import shutil\n",
    "import pyatomdb\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the yields from AGB stars and massive stars\n",
    "file_paths = [\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_C15_LC18_R_mix.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_C15_N13_0_0_HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_C15_N13_0_5_HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_C15_N13_1_0_HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_K06_0.0HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_K06_0.5HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_K06_1.0HNe.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_LC18_R000.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_LC18_R150.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_LC18_R300.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_K10_LC18_Ravg.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_FRUITY.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_K06.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_K10.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_MESAonly_fryer12delay_wind_preexp.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_MESAonly_fryer12mix.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_MESAonly_ye.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_MESAonly_fryer12rapid.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_MESAonly_fryer12delay.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/agb_and_massive_stars_nugrid_N13.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_portinari98_marigo01_gce_totalyields.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_MESA_only.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_MESA_only_fryer12_delay_neutrons.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_MESA_only_fryer12_exclnalpha.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_MESA_only_fryer12_rapid_neutrons.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_MESA_only_ye_neutrons.txt\",\n",
    "    # \"./Original/NuPyCEE/Yields/other/isotope_yield_table_portinari98_marigo01_withg.txt\", \n",
    "    # The last line is \"G\", which is not an element.\n",
    "    \"./Original/NuPyCEE/Yields/other/isotope_yield_table_wiersma09.txt\",\n",
    "]\n",
    "# Selected yields from NuPyCEE, https://github.com/NuGrid/NuPyCEE/tree/master/yield_tables\n",
    "\n",
    "\n",
    "for file_path in file_paths:\n",
    "    out_dir = './NuPyCEE/' + file_path.split('/')[-1].replace(\".txt\", \"\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    shutil.copy(file_path, out_dir)\n",
    "    with open (file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    table_sep = [ ]  # To store the line number where the table starts and ends\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"H Table:\"):\n",
    "            table_sep.append(i)\n",
    "    tables = { }  # To store the tables\n",
    "\n",
    "    for i, line_num in enumerate(table_sep):\n",
    "        Zini = 'Z='+re.search(r\"Z=(\\d+(\\.\\d+)?)\", lines[line_num]).group(1)\n",
    "        if Zini in tables.keys():\n",
    "            pass\n",
    "        else:\n",
    "            tables[Zini] = [ ]\n",
    "        if i == len(table_sep)-1:\n",
    "            tables[Zini].append(lines[line_num:])\n",
    "        else:\n",
    "            tables[Zini].append(lines[line_num:table_sep[i+1]])\n",
    "\n",
    "    # Table of the same initial metallicity are stored in the same list\n",
    "    dfs = { }  # To store the dataframes\n",
    "    mass_lifetime = { }  # To store the mass and lifetime of the stars\n",
    "    for Zini in tables.keys():\n",
    "        if Zini in dfs.keys():\n",
    "            pass\n",
    "        else:\n",
    "            dfs[Zini] = [ ]\n",
    "            mass_lifetime[Zini] = [ ]\n",
    "        for i, table in enumerate(tables[Zini]):\n",
    "            for idx, item in enumerate(table):\n",
    "                if item.startswith(\"&Isotopes\"):\n",
    "                    break\n",
    "            table_data = [line.replace('\\n', '') for line in table[idx:]]\n",
    "            table_data = [line.replace('&', ' ').split() for line in table_data]\n",
    "            df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "            Mrem = re.search(r\"Mfinal: ([+-]?\\d*\\.?\\d+[eE][+-]?\\d+)\", table[2]).group(1)\n",
    "            lifetime = re.search(r\"Lifetime: ([+-]?\\d*\\.?\\d+[eE][+-]?\\d+)\", table[1]).group(1)\n",
    "            Mini = re.search(r\"M=([+-]?\\d*\\.?\\d+[eE]?[+-]?\\d*)\", table[0]).group(1)\n",
    "            mass_lifetime[Zini].append([float(Mini), float(lifetime)])\n",
    "            if 'X0' in df.columns:\n",
    "                df = df.astype({'Isotopes':str, 'Yields':float, 'X0':float, 'Z':int, 'A':int})\n",
    "            else:\n",
    "                df = df.astype({'Isotopes':str, 'Yields':float, 'Z':int, 'A':int})\n",
    "                df['X0'] = 0\n",
    "                df = df[['Isotopes', 'Yields', 'Z', 'X0', 'A']]\n",
    "            df['Isotopes'] = df['Isotopes'].str.replace(r'-\\d+', '', regex=True)\n",
    "            df = df.groupby('Isotopes').sum().reset_index()\n",
    "            # After groupby, the order of columns is changed\n",
    "            df = df[['Isotopes', 'Yields', 'Z', 'X0', 'A']]\n",
    "            df['Z'] = df['Isotopes'].apply(lambda x: pyatomdb.atomic.elsymb_to_Z(x))\n",
    "            # reset_index is a must.\n",
    "            df = df.reset_index(drop=True)\n",
    "            df.loc[len(df)] = ['Mrem', float(Mrem), 0, 0, 0]\n",
    "            # Z=0 for the Mrem is used to keep it in the first row\n",
    "            if float(Mini) - df['Yields'].sum() > 0:\n",
    "                df.loc[len(df)] = [\"Other\", float(Mini) - df['Yields'].sum(), 31, 0, 0]\n",
    "            else:\n",
    "                df.loc[len(df)] = [\"Other\", 0, 31, 0, 0]\n",
    "            # Z=0 is the Mrem\n",
    "            # Z=1, 2, 3, ..., 30 are the first 30 elements\n",
    "            # Z=31 is the rest of the elements\n",
    "            # Drop the isotopes with Z>30\n",
    "            df = df.loc[(df['Z']<=31) & (df['Isotopes']!='Ga')]\n",
    "            df.sort_values(by='Z', inplace=True)\n",
    "            df.rename(columns={'Isotopes':'M', 'Yields':Mini}, inplace=True)\n",
    "            df.drop(columns=['Z', 'X0', 'A'], inplace=True)\n",
    "            df.set_index('M', inplace=True)\n",
    "            dfs[Zini].append(df)\n",
    "    for key in dfs.keys():\n",
    "        # df1 are the dataframes with the same initial metallicity\n",
    "        df1 = pd.concat(dfs[key], axis=1)\n",
    "        df1.to_csv(os.path.join(out_dir, '%s.csv' % key))\n",
    "        # Mass grids where the yields are to be extra or interpolated\n",
    "        columns = [\"%0.6e\" % a for a in np.logspace(np.log10(0.08), np.log10(150), 300)]\n",
    "        # Empty dataframe with the same index as df1\n",
    "        df2 = pd.DataFrame(np.zeros((len(df1.index), len(columns))), index=df1.index, columns=columns)\n",
    "        for i, row in df1.iterrows():\n",
    "            x1 = row.index.to_numpy().astype(float)  # x1 is the initial mass from original yields\n",
    "            y1 = row.to_numpy().astype(float)  # y1 is the remnant mass from original yields\n",
    "            df2.loc[row.name, :] = [utils.extra_interpolate_yields(x1, y1, row.name, float(col)) for col in columns]\n",
    "        df2.to_csv(os.path.join(out_dir, '%s_interpolated.csv' % key))\n",
    "    with h5py.File(os.path.join(out_dir, 'yields1.h5'), 'w') as f:\n",
    "        url = ['https://github.com/NuGrid/NuPyCEE/blob/master/yield_tables/']\n",
    "        url.append(\"/\".join(file_path.split('/')[4:]))\n",
    "        url = \"\".join(url)\n",
    "        f.attrs['OriginalURL'] = url\n",
    "        f.attrs['OriginalFile'] = file_path.split('/')[-1]\n",
    "        for key in dfs.keys():\n",
    "            # Create a group for each initial metallicity\n",
    "            f.create_group(key)\n",
    "            df1 = pd.read_csv(os.path.join(out_dir, '%s.csv' % key))\n",
    "            # hdf5 does not support string with object type\n",
    "            # \"|S\" will find the maximum length of the string in the selected column\n",
    "            df1['M'] = df1['M'].astype('|S')\n",
    "            data = df1.to_records(index=False)\n",
    "            dtype = df1.to_records(index=False).dtype\n",
    "            f[key].attrs['Z'] = \"%s\" % key[2:]\n",
    "            f[key].attrs['MassUnit'] = 'Msun'\n",
    "            f[key].create_dataset('Original', data=data, dtype=dtype)\n",
    "\n",
    "            df2 = pd.read_csv(os.path.join(out_dir, '%s_interpolated.csv' % key))\n",
    "            df2['M'] = df2['M'].astype('|S')\n",
    "            data = df2.to_records(index=False)\n",
    "            dtype = df2.to_records(index=False).dtype\n",
    "            f[key].create_dataset('Interpolated', data=df2.to_records(), dtype=dtype)\n",
    "            f[key].create_dataset('MassLifetime', data=np.array(mass_lifetime[key]))\n",
    "    # Read yields1.h5\n",
    "    # f = h5py.File(os.path.join(out_dir, 'yields1.h5'), 'r')\n",
    "\n",
    "    # yields2.h5 is the other kind of hdf5 file by using to_hdf\n",
    "    # but the content is the same as yields1.h5\n",
    "    with h5py.File(os.path.join(out_dir, 'yields2.h5'), 'w') as f:\n",
    "        url = ['https://github.com/NuGrid/NuPyCEE/blob/master/yield_tables/']\n",
    "        url.append(\"/\".join(file_path.split('/')[4:]))\n",
    "        url = \"\".join(url)\n",
    "        f.attrs['OriginalURL'] = url\n",
    "        f.attrs['OriginalFile'] = file_path.split('/')[-1]\n",
    "    for key in dfs.keys():\n",
    "        df1 = pd.read_csv(os.path.join(out_dir, '%s.csv' % key), index_col=0)\n",
    "        df1.to_hdf(os.path.join(out_dir, 'yields2.h5'), key='/%s/Original' % key.replace(\"=\", \"_\").replace(\".\", \"_\"), mode='a')\n",
    "        df2 = pd.read_csv(os.path.join(out_dir, '%s_interpolated.csv' % key), index_col=0)\n",
    "        df2.to_hdf(os.path.join(out_dir, 'yields2.h5'), key='/%s/Interpolated' % key.replace(\"=\", \"_\").replace(\".\", \"_\"), mode='a')\n",
    "    # Add comments\n",
    "    with h5py.File(os.path.join(out_dir, 'yields2.h5'), 'a') as f:\n",
    "        for key in dfs.keys():\n",
    "            f[key.replace(\"=\", \"_\").replace(\".\", \"_\")].create_dataset('MassLifetime', data=np.array(mass_lifetime[key]))\n",
    "            f[key.replace(\"=\", \"_\").replace(\".\", \"_\")].attrs['Z'] = \"%s\" % key[2:]\n",
    "            f[key.replace(\"=\", \"_\").replace(\".\", \"_\")].attrs['MassUnit'] = 'Msun'\n",
    "    \n",
    "    # Read yields2.h5\n",
    "    # pd.read_hdf(os.path.join(out_dir, 'yields2.h5'), key=\"Z_0_0004/original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../inputs/NuPyCEE'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the yields from SNIa\n",
    "file_paths = [\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_i99_CDD1.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_i99_CDD2.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_i99_W7.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo12_mix_z.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo12_stable_z.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo12_unstable_z.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo13_mix_z.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo13_stable_z.txt\",\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_ivo13_unstable_z.txt\",\n",
    "    # \"./Original/NuPyCEE/Yields/sn1a_t03.txt\",  # The last line is \"G\", which is not an element.\n",
    "    \"./Original/NuPyCEE/Yields/sn1a_t86.txt\"\n",
    "]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    out_dir = './NuPyCEE/' + file_path.split('/')[-1].split('.')[0]\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    shutil.copy(file_path, out_dir)\n",
    "    with open (file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"&Isotopes\"):\n",
    "            break\n",
    "    table_data = [line.replace('\\n', '') for line in lines[idx:]]\n",
    "    table_data = [line.replace('&', ' ').split() for line in table_data]\n",
    "    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].astype(float)\n",
    "    df['Isotopes'] = df['Isotopes'].str.replace(r'-\\d+', '', regex=True)\n",
    "    df = df.groupby('Isotopes').sum().reset_index()\n",
    "    df.rename(columns={'Isotopes':'M'}, inplace=True)\n",
    "    df['Z'] = df['M'].apply(lambda x: pyatomdb.atomic.elsymb_to_Z(x))\n",
    "    df.sort_values(by='Z', inplace=True)\n",
    "    # Drop the isotopes with Z>30\n",
    "    df = df.loc[df['Z']<=30]\n",
    "    df.drop(columns=['Z'], inplace=True)\n",
    "    df.set_index('M', inplace=True)\n",
    "    df.to_csv(os.path.join(out_dir, 'yields.csv'))\n",
    "    dfs = { }\n",
    "    for col in df.columns:\n",
    "        dfs[col] = df[[col]].copy()\n",
    "        dfs[col].to_csv(os.path.join(out_dir, '%s.csv' % col))\n",
    "    \n",
    "    with h5py.File(os.path.join(out_dir, 'yields1.h5'), 'w') as f:\n",
    "        url = ['https://github.com/NuGrid/NuPyCEE/blob/master/yield_tables/']\n",
    "        url.append(\"/\".join(file_path.split('/')[4:]))\n",
    "        url = \"\".join(url)\n",
    "        f.attrs['OriginalURL'] = url\n",
    "        f.attrs['OriginalFile'] = file_path.split('/')[-1]\n",
    "        for key in dfs.keys():\n",
    "            # Create a group for each initial metallicity\n",
    "            f.create_group(key)\n",
    "            df1 = pd.read_csv(os.path.join(out_dir, '%s.csv' % key))\n",
    "            # hdf5 does not support string with object type\n",
    "            # \"|S\" will find the maximum length of the string in the selected column\n",
    "            df1['M'] = df1['M'].astype('|S')\n",
    "            data = df1.to_records(index=False)\n",
    "            dtype = df1.to_records(index=False).dtype\n",
    "            f[key].attrs['Z'] = \"%s\" % key[2:]\n",
    "            f[key].attrs['MassUnit'] = 'Msun'\n",
    "            f[key].create_dataset('Original', data=data, dtype=dtype)\n",
    "    # Read yields.h5\n",
    "    # f = h5py.File(os.path.join(out_dir, 'yields.h5'), 'r')\n",
    "\n",
    "    # yields1.h5 is the other kind of hdf5 file by using to_hdf\n",
    "    # but the content is the same as yields.h5\n",
    "    with h5py.File(os.path.join(out_dir, 'yields2.h5'), 'w') as f:\n",
    "        url = ['https://github.com/NuGrid/NuPyCEE/blob/master/yield_tables/']\n",
    "        url.append(\"/\".join(file_path.split('/')[4:]))\n",
    "        url = \"\".join(url)\n",
    "        f.attrs['OriginalURL'] = url\n",
    "        f.attrs['OriginalFile'] = file_path.split('/')[-1]\n",
    "    for key in dfs.keys():\n",
    "        df1 = pd.read_csv(os.path.join(out_dir, '%s.csv' % key), index_col=0)\n",
    "        df1.to_hdf(os.path.join(out_dir, 'yields2.h5'), key='/%s/Original' % key.replace(\"=\", \"_\").replace(\".\", \"_\"), mode='a')\n",
    "    # Add comments\n",
    "    with h5py.File(os.path.join(out_dir, 'yields2.h5'), 'a') as f:\n",
    "        for key in dfs.keys():\n",
    "            f[key.replace(\"=\", \"_\").replace(\".\", \"_\")].attrs['Z'] = \"%s\" % key[2:]\n",
    "            f[key.replace(\"=\", \"_\").replace(\".\", \"_\")].attrs['MassUnit'] = 'Msun'\n",
    "\n",
    "# Copy the NuPyCEE directory to the inputs directory\n",
    "if os.path.exists('../inputs/NuPyCEE'):\n",
    "    shutil.rmtree('../inputs/NuPyCEE')\n",
    "shutil.copytree('./NuPyCEE', '../inputs/NuPyCEE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In yields1.h\n",
    "- The groups are named as `Z=X.XXXX`, where Z is the initial metallicity\n",
    "- There 3 datasets in each group, named `Origianl`, `Interpolated`, and `MassLifetime`, which are the original yields, the interpolated yields, and the original mass lifetime, respectively.\n",
    "\n",
    "# In yields2.h\n",
    "- The groups are named as `Z_X_XXXX`, where Z is the initial metallicity. According to the convention from PyTable, the group name should be `Z_X_XXXX`.\n",
    "- There 3 datasets in each group, named `Origianl`, `Interpolated`, and `MassLifetime`, which are the original yields, the interpolated yields, and the original mass lifetime, respectively.\n",
    "\n",
    "yield1.h and yield2.h are the same, but the group names are different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
